{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data as dt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "import requests\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "# BASE_DIR will be like '/home/jovyan/DemoExample/'\n",
    "BASE_DIR = pathlib.Path().absolute()\n",
    "print(f\"Working dir: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(url, filename):\n",
    "    # Download file and place it on local storage\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"{filename} downloaded from {url}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"No internet connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/DemoExample/mnist.npz downloaded from https://github.com/sbercloud-ai/aicloud-examples/raw/master/quick-start/mnist.npz\n"
     ]
    }
   ],
   "source": [
    "save_file(\"https://github.com/sbercloud-ai/aicloud-examples/raw/master/quick-start/notebooks_gpu/mnist.npz\", BASE_DIR.joinpath(\"mnist.npz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(BASE_DIR.joinpath('mnist.npz'))\n",
    "mnist_images_train = np.expand_dims(data['x_train'], 1)\n",
    "mnist_labels_train = data['y_train']\n",
    "\n",
    "mnist_images_test = np.expand_dims(data['x_test'], 1)\n",
    "mnist_labels_test = data['y_test']\n",
    "data.close()\n",
    "\n",
    "dataset_train = dt.TensorDataset(torch.Tensor(mnist_images_train), torch.Tensor(mnist_labels_train).long())\n",
    "dataset_test = dt.TensorDataset(torch.Tensor(mnist_images_test), torch.Tensor(mnist_labels_test).long())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=50)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple convnet classifier\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CNNClassifier()\n",
    "device = torch.device(f'cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataParallel if several GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    clf = nn.DataParallel(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d-%H_%M\")\n",
    "writer = SummaryWriter(log_dir=BASE_DIR.joinpath('logs/log_' + current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(clf.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, clf, optimizer, writer):\n",
    "    clf.train()  # set model in training mode (need this because of dropout)\n",
    "\n",
    "    # dataset API gives us pythonic batching\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # forward pass, calculate loss and backprop!\n",
    "        optimizer.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = F.nll_loss(preds, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(f'train loss = {loss.item()}')\n",
    "            writer.add_scalar('Train', loss.item(), epoch * len(train_loader) + batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, clf, writer):\n",
    "    clf.eval()  # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = clf(data)\n",
    "        test_loss += F.nll_loss(output, target).item()\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    \n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    \n",
    "    mlflow.log_metric(\"Test loss\", test_loss)  # add mlflow metrics\n",
    "    mlflow.log_metric(\"Accuracy\", np.round(accuracy.item(),1)) # add mlflow metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train with num epoch = 3\n",
      "Epoch 0\n",
      "train loss = 22.238218307495117\n",
      "train loss = 2.276198148727417\n",
      "train loss = 2.2093944549560547\n",
      "train loss = 1.3249439001083374\n",
      "train loss = 1.3222427368164062\n",
      "train loss = 0.6162148118019104\n",
      "train loss = 0.7235971093177795\n",
      "train loss = 0.3437134623527527\n",
      "train loss = 0.48278796672821045\n",
      "train loss = 0.513648509979248\n",
      "train loss = 0.24412333965301514\n",
      "train loss = 0.5696728229522705\n",
      "\n",
      "Test set: Average loss: 0.2380, Accuracy: 9256/10000 (93%)\n",
      "\n",
      "Epoch 1\n",
      "train loss = 0.3267053961753845\n",
      "train loss = 0.3192499876022339\n",
      "train loss = 0.4160289764404297\n",
      "train loss = 0.08153068274259567\n",
      "train loss = 0.6905174851417542\n",
      "train loss = 0.15559092164039612\n",
      "train loss = 0.3709448277950287\n",
      "train loss = 0.2520889639854431\n",
      "train loss = 0.3144308030605316\n",
      "train loss = 0.32689231634140015\n",
      "train loss = 0.09587522596120834\n",
      "train loss = 0.4002344608306885\n",
      "\n",
      "Test set: Average loss: 0.1533, Accuracy: 9526/10000 (95%)\n",
      "\n",
      "Epoch 2\n",
      "train loss = 0.15486745536327362\n",
      "train loss = 0.3183668255805969\n",
      "train loss = 0.3592469096183777\n",
      "train loss = 0.1257518231868744\n",
      "train loss = 0.47039878368377686\n",
      "train loss = 0.2261769324541092\n",
      "train loss = 0.4318540096282959\n",
      "train loss = 0.17351549863815308\n",
      "train loss = 0.32168111205101013\n",
      "train loss = 0.15349158644676208\n",
      "train loss = 0.13387654721736908\n",
      "train loss = 0.18863151967525482\n",
      "\n",
      "Test set: Average loss: 0.1202, Accuracy: 9627/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "print(f'Start train with num epoch = {num_epochs}')\n",
    "\n",
    "mlflow.set_tracking_uri('file:/home/jovyan/mlruns')\n",
    "mlflow.set_experiment(\"pytorch_tensorboard_mlflow.ipynb\")\n",
    "with mlflow.start_run(nested=True) as run:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        train(epoch, clf, optimizer, writer)\n",
    "        test(epoch, clf, writer)\n",
    "        torch.save(clf.state_dict(), BASE_DIR.joinpath('logs/log_' + current_time + f\"/model_epoch_{epoch}.bin\"))\n",
    "        writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}